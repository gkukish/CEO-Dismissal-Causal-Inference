import pandas as pd
import glob
import os
import warnings

warnings.filterwarnings("ignore", category = FutureWarning)

# Load the S&P 500 companies dataset
sp500list = pd.read_csv('/Users/giorgikukishvili/Documents/Giorgi/Minerva/Capstone/code/Capstone/datasets/sp500list.csv')

# Plot the distribution of GICS Sectors in the S&P 500
sp500list["GICS Sector"].value_counts().plot(kind="barh", title="Distribution of GICS Sectors in S&P500", xlabel="Count", ylabel="Sector", width=0.5)

# Get the path to the directory containing the CSV files

path = r'/Users/giorgikukishvili/Documents/Giorgi/Minerva/Capstone/code/Capstone/datasets/stock_market_data/sp500/csv'

# Use glob to get a list of all CSV files in the directory
all_files = glob.glob(os.path.join(path, "*.csv"))

# Create an empty DataFrame to store the concatenated data
df = pd.DataFrame()

# Iterate through each file, read the data, and append it to the DataFrame
for file in all_files:
    # Extract the company name from the file name (you may need to adjust this based on your file naming convention)
    company_name = os.path.splitext(os.path.basename(file))[0]
    stock_data = pd.read_csv(file)
    stock_data['Company'] = company_name
    df = df.append(stock_data, ignore_index=True)


df.head()
unique_companies = df['Company'].nunique() #409 companies in total out of 500 companies in S&P500
print(f"\nNumber of Unique Companies: {unique_companies}")
number = df["Company"].value_counts()
number
df_sorted = df.sort_values(by=['Company',"Date"], ascending=True)


df_sorted.info()
